import { ssrRenderAttrs } from "vue/server-renderer";
import { useSSRContext } from "vue";
import { _ as _export_sfc } from "./plugin-vue_export-helper.cc2b3d55.js";
const __pageData = JSON.parse(`{"title":"The Simple Way to Scrape an HTML Table: Google Docs","description":"Raw data is the best data, but a lot of public data can still only be found in tables rather than as directly machine-readable files. One example is the FDIC's List of Failed Banks. Here is a simple trick to scrape such data from a website: Use Google Docs.","frontmatter":{"title":"The Simple Way to Scrape an HTML Table: Google Docs","description":"Raw data is the best data, but a lot of public data can still only be found in tables rather than as directly machine-readable files. One example is the FDIC's List of Failed Banks. Here is a simple trick to scrape such data from a website: Use Google Docs.","date":"2009-11-15T09:59:08.000Z","tags":"data","featuredImage":"https://media.eagereyes.org/wp-content/uploads/2009/11/google-import.png","outline":false},"headers":[],"relativePath":"blog/2009/scrape-tables-using-google-docs.md","filePath":"blog/2009/scrape-tables-using-google-docs.md"}`);
const _sfc_main = { name: "blog/2009/scrape-tables-using-google-docs.md" };
function _sfc_ssrRender(_ctx, _push, _parent, _attrs, $props, $setup, $data, $options) {
  _push(`<div${ssrRenderAttrs(_attrs)}><p align="center"><img src="https://media.eagereyes.org/wp-content/uploads/2009/11/google-import.png" width="825" height="510"></p><h1 id="the-simple-way-to-scrape-an-html-table-google-docs" tabindex="-1">The Simple Way to Scrape an HTML Table: Google Docs <a class="header-anchor" href="#the-simple-way-to-scrape-an-html-table-google-docs" aria-label="Permalink to &quot;The Simple Way to Scrape an HTML Table: Google Docs&quot;">​</a></h1><p><a href="/data/dish-best-served-raw.html">Raw data is the best data</a>, but a lot of public data can still only be found in tables rather than as directly machine-readable files. One example is the FDIC&#39;s <a href="http://www.fdic.gov/bank/individual/failed/banklist.html">List of Failed Banks</a>. Here is a simple trick to scrape such data from a website: Use Google Docs.</p><p>The table on that page is even relatively nice because it includes some JavaScript to sort it. But a large table with close to 200 entries is still not exactly the best way to analyze that data.</p><p>After some digging around – and even considering writing my own throw-away extraction script –, I remembered having read something about <a href="http://docs.google.com/">Google Docs</a> being able to <a href="http://ouseful.wordpress.com/2008/10/14/data-scraping-wikipedia-with-google-spreadsheets/">import tables from websites</a>. And indeed, it has a very useful function called <em><a href="http://docs.google.com/support/bin/answer.py?hl=en&amp;answer=75507">ImportHtml</a></em> that will scrape a table from a page.</p><p>To extract a table, create a new spreadsheet and enter the following expression in the top left cell:</p><pre>=ImportHtml(<em>URL</em>, &quot;table&quot;, <em>num</em>)</pre><p><em>URL</em> here is the URL of the page (between quotation marks), &quot;table&quot; is the element to look for (Google Docs can also import lists), and <em>num</em> is the number of the element, in case there are more on the same page (which is rather common for tables). The latter supposedly starts at 1, but I had to use 0 to get it to pick up the correct table on the FDIC page.</p><p>Once this is done, Google Docs retrieves the data and inserts it into the spreadsheet, including the headers. The last step is to download the spreadsheet as a CSV file.</p><p>This is very simple and quick, and a much better idea than writing a custom script. Of course, the real solution would be to offer all data as a CSV file in addition to the table to begin with. But until that happens, we will need tools like this to get the data into a format that is actually useful.</p><p><em>Posted by <a href="/about">Robert Kosara</a> on November 15, 2009</em></p><aside class="comments"><hr><h2 id="comments" tabindex="-1">Comments <a class="header-anchor" href="#comments" aria-label="Permalink to &quot;Comments&quot;">​</a></h2><p><a href="http://peltiertech.com/WordPress/" rel="nofollow noopener" target="_blank">Jon Peltier</a> says…</p><blockquote><p>It took under a minute to set up a web query in Excel to extract the data. Data menu &gt; Import External Data &gt; New Web Query. Enter the URL, then select the table you want imported. Whenever you want, you can click on the imported table. The External Data toolbar pops up, and you can click on the icon with the exclamation point to update the query. It&#39;s a simple matter to save the sheet with the data as a CSV.</p></blockquote><p><a href="http://www.treelab.org" rel="nofollow noopener" target="_blank">Enrico Poli</a> says…</p><blockquote><p>This is a very nice trick. For more complex data extraction needs, I’ve been playing with Open Dapper (<a href="http://www.dapper.net/open/">http://www.dapper.net/open/</a>) and I found it quite powerful: it’s able to extract data from almost any page showing any kind of regularity – not just tables – and export it at least as csv or rss (and you can access the rss from an url which stays live, updating the feed as the original page update etc.)</p><p>(I’ve searched the site for “Dapper” and the search engine returns no results, so I thought it could be useful to point it to you – thanks for this blog and for your work!)</p></blockquote><p><a href="http://ouseful.info" rel="nofollow noopener" target="_blank">Tony Hirst</a> says…</p><blockquote><p>Hi</p><p>Some time ago I posted a walkthrough of how to scrape data from Wikipedia with Google docs, and then annotate the CSV output of the scraped data with geodata in Yahoo Pipes:</p><p><a title="Permanent Link to &quot;Data Scraping Wikipedia with Google Spreadsheets&quot;" href="http://ouseful.wordpress.com/2008/10/14/data-scraping-wikipedia-with-google-spreadsheets/">Data Scraping Wikipedia with Google Spreadsheets</a></p></blockquote><p>Robert Kosara says…</p><blockquote><p>Excel for the Mac doesn&#39;t have that function, though. </p></blockquote><p>Robert Kosara says…</p><blockquote><p>I haven&#39;t seen that one before, I&#39;ll have to try it out.</p></blockquote><p>Robert Kosara says…</p><blockquote><p>That&#39;s probably where I saw this. I actually did this a while ago and couldn&#39;t find the page where I had seen the ImportHtml trick when I wrote the posting. I&#39;ll add a link.</p></blockquote><p><a href="http://data.timgraham.net" rel="nofollow noopener" target="_blank">Tim</a> says…</p><blockquote><p>I would just use yahoo pipes for this. Very easy (with a small investment of time to learn), and very flexible too. Great tool for scraping.</p></blockquote><p><a href="http://i-ocean.blogspot.com/" rel="nofollow noopener" target="_blank">derek</a> says…</p><blockquote><p>I would be very grateful for a reliable way to scrape PDF tables. </p></blockquote><p>Bill Dedman says…</p><blockquote><p>Why not copy and paste this table into Excel. The rule: If it&#39;s a table, just paste it.</p></blockquote><p>Robert Kosara says…</p><blockquote><p>The good thing about HTML is that tables are very clearly structured in the markup. That&#39;s not the case in PDF, where it&#39;s just stuff that happens to line up and maybe lines that are drawn in-between. Best option is probably to copy&amp;paste from the PDF into Excel or another spreadsheet app.</p></blockquote><p><a href="http://chartsgraphs.wordpress.com" rel="nofollow noopener" target="_blank">D Kelly O&#39;Day</a> says…</p><blockquote><p>Let&#39;s combine your climate data and web scrapping posts.</p><p>Here&#39;s an example where I scrapped climate science data for a visualization of the IR absorption properties of 5 greenhouse gases.</p><p></p><p align="center"><img src="http://chartsgraphs.files.wordpress.com/2009/12/nist_5_ghg_spectra1.png" alt="" width="539" height="538"></p><p>I wanted to scrap the spectrum data from 5 NIST Chemistry Webbook data web pages and generate this chart automatically.</p><p>Jon Peltier is right about Excel&#39;s external data capabilities, however, Jon would need a VBA procedure to retreive the data and reproduce my 5 panel chart.</p><p>Bill Dedman&#39;s suggestion about using Excel&#39;s cut and paste approach would be quite time consuming and Bill would have a challenge generating the 5 panel chart.</p><p>Here&#39;s a link to my post, it includes a link to my R script, available on Google docs.</p><p>http://chartsgraphs.wordpress.com/2009/12/07/understanding-the-science-of-co2%E2%80%99s-role-in-climate-change-3-%E2%80%93-how-green-house-gases-trap-heat/</p><p></p><p></p></blockquote><p>Jay says…</p><blockquote><p>using something like HTML::TableContentParser or HTML::TableExtract and a cron job if i needed to keep it up to date.</p></blockquote><p><a href="http://androidgeeky.weebly.com" rel="nofollow noopener" target="_blank">David G.</a> says…</p><blockquote><p>Awesome feature.. great tutorial--</p><p>Thanks</p><p>-- androidgeeky.weebly.com</p></blockquote><p>ftr says…</p><blockquote><p>For Firefix users there is the Table2Clipboard extension that works well here, at <a href="https://addons.mozilla.org/fr/firefox/addon/dafizilla-table2clipboard/" target="_blank" rel="noreferrer">https://addons.mozilla.org/fr/firefox/addon/dafizilla-table2clipboard/</a></p><p>And there is TableTools2, too, at <a href="http://datatables.net/forums/discussion/3726/tabletools-2-released/p1" target="_blank" rel="noreferrer">http://datatables.net/forums/discussion/3726/tabletools-2-released/p1</a></p></blockquote><p>kristineh says…</p><blockquote><p>This is a great website, but you have some syntax errors:</p><p>=ImportHtml(URL, “table”, num)</p><p>should be: =ImportHtml(URL; “table”; num)</p></blockquote><p>Christopher says…</p><blockquote><p>I tried the method you described, I keep getting a parse error. Any idea what I am doing wrong?</p></blockquote><p>JD Markland says…</p><blockquote><p>The table to clipboard method is by far the easiest. Thanks for the great posts!</p></blockquote><p>ftr says…</p><blockquote><p>I ocr them with FineReader</p></blockquote><p><a href="http://perfectnewgadgets.blogspot.com/" rel="nofollow noopener" target="_blank">TechView</a> says…</p><blockquote><p>You could have used MS Excel&#39;s &quot;Data--&gt;From Web&quot; option to fetch tables from web sites. It&#39;s more easy and workable. And it could be updated automatically also when spreadsheet opens every time. Cheers...</p></blockquote><p><a href="http://gravatar.com/marktbullock" rel="nofollow noopener" target="_blank">Mark Bullock</a> says…</p><blockquote><p>@TechView - this doesn&#39;t work for Office for Mac 2011. The work-around is to construct a query and then &quot;Get External Data&quot;, &quot;Run Saved query&quot;.</p></blockquote><p>Kevin says…</p><blockquote><p>Hi, good post, how do you find the table number in this example? tks</p></blockquote><p>Kevin says…</p><blockquote><p>@ Jon Peltier --&gt; How do I do this in excel 10, if I use this <a href="http://www.fdic.gov/bank/individual/failed/banklist.html" target="_blank" rel="noreferrer">http://www.fdic.gov/bank/individual/failed/banklist.html</a> as an example it does not work, it just imports the whole page and not just the table as the google docs does. So Google Docs is alot better from what i have seen....</p></blockquote><p><a href="http://twitter.com/lyda" rel="nofollow noopener" target="_blank">Kevin Lyda (@lyda)</a> says…</p><blockquote><p>Just curious, why all the defensive posts from Excel users?</p></blockquote><p><a href="http://gravatar.com/wguteamsmith" rel="nofollow noopener" target="_blank">wguteamsmith</a> says…</p><blockquote><p>i&#39;m a newbie with complex ideas and no programming skills. i need to extract table data from an internal web portal page. i can&#39;t find the table id. i&#39;m in firefox and viewed the frame source. all i can find is a table class. thoughts? ideas? if i can make this work, i&#39;ve just saved myself hours per week.</p></blockquote><p><a href="http://www.reconversionprofessionnelle.org/" rel="nofollow noopener" target="_blank">arthur huppe</a> says…</p><blockquote><p>Awesome. I bought a list of directories and I would like to import it to my google doc account. This article helped me. Thank you so much.</p></blockquote><p><a href="http://skyul.com" rel="nofollow noopener" target="_blank">Alexandru Cobuz</a> says…</p><blockquote><p>I didn&#39;t even know that Excel was capable of this. Great tip! Ha, even a beginner could scrape some data with this.</p></blockquote><p>Jason Pittman says…</p><blockquote><p>Another good way to import data into a spreadsheet is from JSON data. If you use json-csv.com you can upload text or enter a URL and a spreadsheet will be produced.</p></blockquote><p>Alex says…</p><blockquote><p>Data menu &gt; Import External Data &gt; New Web Query</p><p>just find this today....i was doing it by hand for years.....thanks man!</p></blockquote><p><a href="http://www.spraycreek.ca/" rel="nofollow noopener" target="_blank">tbanwell</a> says…</p><blockquote><p>This was a great help for me; using it to pull cattle price data. Using &#39;Table 0&#39; was the tip that saved me. Thanks!</p></blockquote><p>Bob says…</p><blockquote><p>Minor error - code needs quotes around URL, like this =ImportHtml(&quot;<a href="http://example.com/" target="_blank" rel="noreferrer">http://example.com/</a>&quot;, &quot;table&quot;, 0)</p></blockquote><p><a href="http://blog.tkacprow.pl" rel="nofollow noopener" target="_blank">Tom</a> says…</p><blockquote><p>Thanks definitely useful - I didn&#39;t know Google docs had this feature. I approached this issue however differently making my own Add-In and UDF functions so that there would be no need of writing VBA as in the Google example:</p><p><a href="http://blog.tkacprow.pl/excel-tools/excel-scrape-html-add/" target="_blank" rel="noreferrer">http://blog.tkacprow.pl/excel-tools/excel-scrape-html-add/</a></p><p>Gettting a href is easy: =GetElementByRegex(&quot;[URL]&quot;;&quot;href=&quot;&quot;(.*?)&quot;&quot;)</p><p>A more elaborate example as yours above, of getting the tables contents in Excel, is also not that complicated and just requires nesting of the UDF: =GeRegex( GetRegex( GetElementByRegex(&quot;[URL]&quot;;&quot;([^&quot;&quot;]<em>?)&quot;;0); &quot;(?😦.|\\n)</em>?)&quot;;0); &quot;(?😦.|\\n)*?)&quot;;0)</p><p>gets the first cell contents of the first row and the first table. Changing the &quot;0&quot; to higher numbers will get the next rows, cells etc. so the formulas can be dragged like other excel formulas 😃.</p><p>Hope this will interest you!</p></blockquote><p><a href="http://gravatar.com/grantkaye" rel="nofollow noopener" target="_blank">Grant Kaye</a> says…</p><blockquote><p>Hi Robert, thanks for this brief tutorial. I&#39;m wondering if your technique can be applied to automatically gather weather sensor data once a day at a specific time and feed the data into a Google Doc spreadsheet? For example: <a href="http://weather.uwyo.edu/cgi-bin/wyowx.fcgi?TYPE=SFLIST&amp;amp;DATE=CURRENT&amp;amp;HOUR=CURRENT&amp;amp;UNITS=A&amp;amp;&amp;amp;STATION=PACV" target="_blank" rel="noreferrer">http://weather.uwyo.edu/cgi-bin/wyowx.fcgi?TYPE=SFLIST&amp;amp;DATE=CURRENT&amp;amp;HOUR=CURRENT&amp;amp;UNITS=A&amp;amp;&amp;amp;STATION=PACV</a></p></blockquote><p><a href="/about" rel="nofollow noopener" target="_blank">Robert Kosara</a> says…</p><blockquote><p>No, because the table on that page is just raw text, not HTML. I don&#39;t know if something like <a href="https://import.io" rel="nofollow">import.io</a> would work, but worth a try. You&#39;d need to extract the text table and then run that through something that can parse tab-separated data, like Google Refine or Data Wrangler.</p></blockquote><p><a href="https://www.facebook.com/magicjuand" rel="nofollow noopener" target="_blank">Juan J. Pérez</a> says…</p><blockquote><p>I was wondering if there is a way to scrape only certain rows?</p></blockquote><p><a href="https://www.facebook.com/magicjuand" rel="nofollow noopener" target="_blank">Juan J. Pérez</a> says…</p><blockquote><p>also, if I can select a column only when a cell matches the criteria, for example, I&#39;m doing a data scraping from the fire rescue website, on that table there are some rows I dont want on my page and I only want calls from a certain unit number.<a href="http://www.orangecountyfl.net/EmergencySafety/FireRescueActiveCalls.aspx#.VLlBL8bnjKA" target="_blank" rel="noreferrer">http://www.orangecountyfl.net/EmergencySafety/FireRescueActiveCalls.aspx#.VLlBL8bnjKA</a></p></blockquote><p><a href="/about" rel="nofollow noopener" target="_blank">Robert Kosara</a> says…</p><blockquote><p>Not on import, but once the data is imported, you can sort, filter, etc. in Google Docs. Click a column header to highlight it, then pick Data-&gt;Filter from the menu. When you then click on the first row in that column, you get a dropdown that lets you filter, etc.</p></blockquote><p><a href="http://alexgerdom.wordpress.com" rel="nofollow noopener" target="_blank">alexgerdom</a> says…</p><blockquote><p>Tabula ( <a href="http://tabula.Technology/" target="_blank" rel="noreferrer">http://tabula.Technology/</a> ) may work for you. I haven&#39;t had too much time to play around with it yet, but it shows some promise. The PDF would need to be OCR&#39;d first, and they say it still struggles with headers. But if you have a really dense table that needs scraped, I&#39;m sure there are worse things out there.</p></blockquote><p>Craig says…</p><blockquote><p>The page I want to scrape has a submit button on it. Is there a way to get the form to submit and use this method? The default form information is all I need from the table.</p></blockquote><p>Nate says…</p><blockquote><p>Robert, This is a great resource, thank you. I&#39;ve been trying to grab the table data from this website: <a href="http://som.yale.edu/faculty-research/our-centers-initiatives/international-center-finance/data/stock-market-confidence-indices/united-states-valuation-index" target="_blank" rel="noreferrer">http://som.yale.edu/faculty-research/our-centers-initiatives/international-center-finance/data/stock-market-confidence-indices/united-states-valuation-index</a></p><p>The Googlesheets function =importHTML(&quot;<a href="http://www.spindices.com/indices/real-estate/sp-case-shiller-20-city-composite-home-price-index%22,%22table" target="_blank" rel="noreferrer">http://www.spindices.com/indices/real-estate/sp-case-shiller-20-city-composite-home-price-index&quot;,&quot;table</a>&quot;,0) returns the #N/A &quot;imported content is empty&quot; error, as does changing the index to 1, or 2, etc.</p><p>I thought I might be able to use the importXML function, and the HTML from the site i&#39;m attempting to retrieve data from shows:</p><pre><code>        Valuation Index - Institutional
        
          Date
          Index Value
          Standard Error
</code></pre><p>Oct 1989 80.58 3.36</p><p>And so on.</p><p>I included the start of the data I want from the first row for Oct 1989. Since the table doesn&#39;t appear to have an id, I&#39;ve tried are &quot;//table/&quot;, &quot;//td&quot;, &quot;//td/&quot;, &quot;//div/&quot; and can&#39;t get the function to return anything so it appears I&#39;m missing some fundamental concept. This can&#39;t be that hard. Do you have any suggestions?</p></blockquote><p>Kevin Stirtz says…</p><blockquote><p>Hi John - Just curious if Excel can scrape multiple URLs this way. Thanks</p></blockquote><p>SGIII says…</p><blockquote><p>Just wanted to chime in to say how helpful this is, Robert.</p></blockquote><p>aestuehler says…</p><blockquote><p>I have read all the posts and tried a variety of approaches but I am not able to extract the data I am looking for with these methods. The website appears to not have a table or list behind the data so that may be the problem - I am not sure. I am looking to extract all of the camps listed in the New England states (CT, ME, MA, NH, RI, VT) from this site: find.acacamps.org. Using the excel approach returns no data (after I select the table created after the search). Google sheets just returns the text on the screen from either a table (0 - 4) or list.</p><p>Any other suggestions please?</p></blockquote><p>Bob G Grieger says…</p><blockquote><p>I captured the first table OK. How do I automatically capture the next 50 pagination sheets?</p><p>Thanks for this article.</p></blockquote><p>Josh says…</p><blockquote><p>I think I&#39;m having the same issue as Bob above. I&#39;m pulling stats from here: <a href="http://www.cbssports.com/collegebasketball/stats/teamsort/NCAAB/DEFENSIVE/regularseason/yearly" target="_blank" rel="noreferrer">http://www.cbssports.com/collegebasketball/stats/teamsort/NCAAB/DEFENSIVE/regularseason/yearly</a></p><p>The default is for the page to load the first 50ish rows and then you have to select an option at the bottom to turn to the next page or view all records (neither of which are at a different URL). Any way to use this or other tools to work around this issue?</p><p>Thanks!</p></blockquote><p>SK says…</p><blockquote><p>Brilliant one ! Thank you 😃</p><p>I improvised a little on it. My challenge was to have information extracted from multiple urls present on one url.</p><p>So with the help of data miner I got all urls extracted on one google spreadsheet, then using importhtml, instead of url, I gave cell address where urls are present to fetch data.</p><p>This has worked well for me.</p><p>Only challenge is the field of email id coming as protected whereas I can see email id mentioned on webpage. This is something still not resolved. Any view?</p></blockquote><p>glennhenshaw says…</p><blockquote><p>This is good to know.</p><p>You can also use python&#39;s pandas library</p><ol><li>highlight the table</li><li>import pandas as pd</li><li>table = pd.read_clipboard()</li></ol></blockquote><p>Holden says…</p><blockquote><p>It is a manual process. Get the url of the 50-100 result set and use it in the next cell or sheet of the G sheets.</p><p>I too would be interested if there was a simpler method.</p></blockquote><p><a href="http://astapramuditya.wordpress.com" rel="nofollow noopener" target="_blank">astapramuditya</a> says…</p><blockquote><p>Thanks!</p></blockquote><p>Peter says…</p><blockquote><p>Hi,</p><p>Do you know the method (maybe by use of the script) how to load the content of the website in case when it is loaded dynamically (ajax)?</p><p>Thanks, Peter</p></blockquote><p>Alexandro Trese says…</p><blockquote><p>Just saved me hours of time, thanks!</p></blockquote><p>Jennifer Jennifer says…</p><blockquote><p>You mean Google Sheets. Google Docs is a word processor, like MS Word.</p></blockquote><p><a href="https://www.commentmaison.com/" rel="nofollow noopener" target="_blank">Jérémy</a> says…</p><blockquote><p>That&#39;s what I was looking for ! So simple and so helpful. thank you</p></blockquote><p>Gonçalo Couto says…</p><blockquote><p>Thanks for sharing. Great trick!</p></blockquote><p>Rahul says…</p><blockquote><p>how to know the table numbers. some tables are easily appearing by trying 1,2,3 but some are not</p></blockquote><p>Ryan Stoddard says…</p><blockquote><p>exactly... I have been trying to scrape the &quot;miscellaneous stats&quot; table from Basketball Reference for a while now... the tables available are 1-6 and anything after that it comes up N/A.... maybe the table is labeled a random number?</p></blockquote></aside></div>`);
}
const _sfc_setup = _sfc_main.setup;
_sfc_main.setup = (props, ctx) => {
  const ssrContext = useSSRContext();
  (ssrContext.modules || (ssrContext.modules = /* @__PURE__ */ new Set())).add("blog/2009/scrape-tables-using-google-docs.md");
  return _sfc_setup ? _sfc_setup(props, ctx) : void 0;
};
const scrapeTablesUsingGoogleDocs = /* @__PURE__ */ _export_sfc(_sfc_main, [["ssrRender", _sfc_ssrRender]]);
export {
  __pageData,
  scrapeTablesUsingGoogleDocs as default
};
